---
documentclass: article
fontsize: 12pt
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: 
    toc: false
    latex_engine: xelatex
    fig_caption: yes
    includes:
      in_header: preamble.sty
      before_body: titlepage.sty
bibliography: references.bib  
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven) # to read SAS files
library(kableExtra)
library(qwraps2)
library(rprojroot)
library(patchwork)
library(psych)
library(gtools)
library(ordinal)

knitr::opts_chunk$set(echo = TRUE)
```

```{r load, include=FALSE, message=FALSE, warning=FALSE}
trichotomization <- c(-100, 6, 25, 120)
data <- read_sas(find_root_file("data/hearing500lr.sas7bdat",
                                criterion = has_file("LDA.Rproj"))) %>%
  mutate(side = as.factor(side),
         side_integer = as.integer(side),
         TIME_scale = unname(scale(TIME, center = TRUE, scale = TRUE)),
         id = as.factor(id),
         id_integer = as.integer(id),
         age_measurement = age + TIME,
         age_scale = unname(scale(age, center = TRUE, scale = TRUE)),
         age_discrete = cut(age,
                            breaks = c(0,30,50,70,100),
                            labels = c("<30", "30-50", "50-70",">70")),
         y_discrete = factor(as.factor(cut(y,
                          breaks = trichotomization,
                          labels = c("Excellent", "Normal", "Hearing loss"))),
                          levels = c("Excellent", "Normal", "Hearing loss"),
                          ordered = TRUE),
         y_integer = as.integer(y_discrete),
         learning = 1*(TIME == 0)) %>%
  arrange(id)
mediansplit <- cut_number(data$y, n = 3)

expit <- function(x){1/(1 + exp(-x))}
```

# Data trichotomization

To trichotomize the data, suitable cut-off points need to be found. The cut-off points are often chosen based on either  expert knowledge or so as to optimize predictive power. An easy, often used method for dichotomization is a median-split since it assures that there are an equal amount of observation at either side of the cut-off value. Similarly, for trichotomization, we could aim for approximately 33.33% of the observations in each of the three categories. That would result in the following three categories: `r sprintf("%s, %s, %s",levels(mediansplit)[1],levels(mediansplit)[2],levels(mediansplit)[3])`.

It is quite common in literature to dichotomize hearing loss into normal hearing (\leq25 dB) and hearing loss (>25 dB) [see @garinis2017cumulative; @gallagher2019; @ju2022long, for example]. However, thichotomization is less common and it should be noted that it is generally not advised to discretize continuous data since some information is inevitably lost [@doi:10.1080/03610926.2016.1248783; @maccallum2002practice]. 

[The Centers for Disease Control and Prevention](https://www.cdc.gov/niosh/mining/userfiles/works/pdfs/2008-102.pdf) distinguishes the following levels of hearing loss, based on @clark1981:

- \leq 25 dB: Normal hearing
- 26 - 40 dB: Mild hearing loss
- 41 - 55 dB: Moderate hearing loss
- 56 - 70 dB: Moderate / severe hearing loss
- 71 - 90 dB: Severe hearing loss
- \geq 91 dB: Profound hearing loss


```{r clark, echo=FALSE, message = FALSE, fig.cap= "Hearing threshold over time, divided by left and right ear. The numbers in the bottom show the number of measurements that were taken.", fig.width = 5, fig.height=4}
data_clark <- data %>% mutate(
  y_clark = as.factor(cut(y, breaks = c(-13, 25, 40, 55, 56, 70, 90, 120)))
) %>%
  group_by(y_clark) %>%
  summarize(n = n(),
            n_id = n_distinct(id),
            avg_age = mean(age_measurement),
            nperc = n/nrow(data)*100) %>%
  ungroup() %>% 
  mutate(Cum = cumsum(n)/sum(n)*100) %>%
  dplyr::select(y_clark, n, nperc, Cum, n_id, avg_age) 
data_clark %>%
  kable(col.names = c("Category", "Nb observations", "Percentage", 
                      "Cumulative percentage", "Nb subjects", "Avg age"),
        caption = "Number of observations in each pre-defined categories from Clark (1981).",
        booktabs = TRUE,
        digits = 2) %>%
  kable_styling(latex_options = "HOLD_position")
```

Table \@ref(tab:clark) shows that, in this dataset, there is no one in the severe hearing loss categories and the large majority has normal hearing (`r round(data_clark[1, "nperc"],2)`%). The median for all observation with normal hearing (\leq 25dB) is 6 dB. We therefore suggest to trichotomize the data into the following categories (Table \@ref(tab:ydiscrete)):

- \leq 6 dB: Excellent hearing
- 7 - 25 dB: Normal hearing
- \geq 25 dB: Hearing loss

```{r ydiscrete, echo=FALSE, message = FALSE, fig.cap= "Hearing threshold over time, divided by left and right ear. The numbers in the bottom show the number of measurements that were taken.", fig.width = 5, fig.height=4}
data %>% 
  group_by(y_discrete) %>%
  summarize(n = n(),
            n_id = n_distinct(id),
            avg_age = mean(age_measurement),
            nperc = n/nrow(data)*100) %>%
  ungroup() %>% 
  mutate(Cum = cumsum(n)/sum(n)*100) %>%
  dplyr::select(y_discrete, n, nperc, Cum, n_id, avg_age) %>%
  kable(col.names = c("Category", "Nb observations", "Percentage", 
                      "Cumulative percentage", "Nb subjects", "Avg age"),
        caption = "Number of observations in each category.",
        booktabs = TRUE,
        digits = 2) %>%
  kable_styling(latex_options = "HOLD_position")
```

# Methodology

As discussed in the previous section, the dependent variable will be split up into three categories. As such, the dependent variable is tranformed from a continuous (integer) variable into an ordinal one where excellent hearing is the lowest level and hearing loss is the highest.

All analysis was done in R. All scripts are freely available at [this git repository](https://github.com/raiisac/LDA). 


# Results

## Marginal model
First, we fit a marginal model with the *ordLORgee* from the **multgee** package. This function allows for an ordinal dependent variable which is appropriate for our data. 

We conduct the GEE analysis in two steps as in @touloumis2014r. First we selected a structure for the marginalized local odds ratios. The full specification is given by 
$$\log \theta_{tjt'j'}=\phi^{(t,t')}(\mu_{j}^{t,t'}-\mu_{j+1}^{t,t'})(\mu_{j'}^{t,t'}-\mu_{j'+1}^{t,t'})$$ 
where $\{\mu_{j}^{t,t'};j=1\dots J\}$ are the score parameters for the $J$ response at the time pair $\{t,t'\}$ and $\phi^{(t,t')}$ is the intrinsic parameter. We nevertheless chose to select a uniform structure i.e. just a fixed $\phi$. Both a categorically exchangeable structure (i.e $\phi^{\{t,t'\}}$) and a time exchangeable structure (i.e.$\phi(\mu_{j'}-\mu_{j'+1})$) gave estimates that are very close to a constant. The final log local odds ratios have the following form where the size of the matrix depends on the number of observations per subject.

$$\log \theta_{tjt'j'} = 
\begin{pmatrix}
0 & 0 & \phi & \phi & \cdots & \phi & \phi \\
0 & 0 & \phi & \phi &\cdots & \phi & \phi \\
\phi & \phi & 0 & 0 &\cdots & \phi & \phi \\
\phi & \phi & 0 & 0 &\cdots & \phi & \phi \\
\vdots  & \vdots & \vdots & \vdots  & \ddots & \vdots & \vdots  \\
\phi & \phi & \phi & \phi & \cdots &  0 & 0 \\
\phi & \phi & \phi & \phi & \cdots &  0 & 0
\end{pmatrix}$$

Where the matrix is dimensions of the per subject observations and the 4 zero block on the diagonal comes up because at each time step we have both a left and a right ear. The side of the ear comes up non-significant. \newline
The next step is model selection which we do with the help of Wald test. We conducted a greedy model selection (both forward by starting with a minimal model and adding variables, and backwards by starting with a full model and removing variables) which fortunately converged. The resulting model was the same as in the model in the previous assignment (model specifications in equation \@ref(eq:Ordinal)). 

\begin{equation}
\left\{
                \begin{array}{ll}
                  logit[P(Y_i\leq Excellent|x_i)] = \alpha_1 + \beta_{1}age_i +  \beta_{2}TIME_i + \beta_{3}learning_i + \\ \hspace{5cm} \beta_{4}age^2_i + \beta_{5}age_i*TIME_i \\
                  logit[P(Y_i\leq Normal|x_i)] = \alpha_2 + \beta_{1}age_i + \beta_{2}TIME_i + \beta_{3}learning_i + \\ \hspace{5cm} \beta_{4}age^2_i + \beta_{5}age_i*TIME_i 
                \end{array}
              \right.
(\#eq:Ordinal)
\end{equation}



```{r loadgee, echo=FALSE, message = FALSE, warning = FALSE, eval = file.exists("Stefan RMD/fit.Rdata")}
#if the R object exists, load it to avoid long calculation times. If not, fit the model on the fly and same the object
library(multgee)
load("Stefan RMD/fit_gee.Rdata")
```

```{r fitgee, echo=FALSE, message = FALSE, eval = !file.exists("Stefan RMD/fit.Rdata")}
#if the R object exists, load it to avoid long calculation times. If not, fit the model on the fly and same the object
library(multgee)
fit <- ordLORgee(formula = y_discrete ~ age*TIME + I(age^2) +learning,
                 link = "logit", id = id, data = data,
                 LORstr = "uniform")#category.exch"
```

```{r geetable, echo=FALSE, message = FALSE}
sgee <- summary(fit)$coefficients 
sgee <- sgee[c(1:4,6,5, 7),] #swap learning and age squared to be the same as in the mixed model
sgee %>% as.data.frame %>%
  mutate(var = c("$\\alpha_1$", "$\\alpha_2$", rownames(sgee)[3:5], 
                 "age$^2$", rownames(sgee)[7]),
         estimate = sprintf("%.2f %s", Estimate, stars.pval(`Pr(>|san.z|)`)),
         odds = round(exp(Estimate),2)) -> sgee
sgee %>% dplyr::select(var, estimate, odds) %>%
  kable(booktabs = TRUE,
        caption = "Estimated GEE model",
        row.names = FALSE,
        escape = FALSE,
        format = "latex",
        col.names = c("Parameter", "Estimate", "Odds"))
```

The result is shown in Table \@ref(tab:geetable). Figure \@ref(fig:geepredictions) shows the marginal probabilities for different age categories. The odds in Table \@ref(tab:geetable) are calculated as $exp(\beta_i)$. As an example for learning, $exp($`r round(sgee[sgee$var =="learning", "Estimate"],2)` $)=$ `r round(sgee[sgee$var =="learning", "odds"],2)` which means that when $TIME==0$, subjects have a `r (1-sgee[sgee$var =="learning", "odds"])*100`% lower odds of having better hearing (excellent versus normal or normal versus hearing loss).

```{r geepredictions, echo=FALSE, message = FALSE, fig.width = 12, fig.height = 7, fig.cap = "Predictions from the marginal model."}
agecat <- c(20,30,40,50,60,70)
nDF <- expand.grid(age = agecat,
                   TIME = seq(0, 22, length.out = 60)) %>%
  mutate(learning = (TIME == 0),
         `I(age^2)` = age^2,
         `age:TIME` = age*TIME) %>%
  dplyr::select(names(coef(fit))[-c(1,2)]) 
#age:TIME is not yet in fit

nDF <- expand.grid(age = agecat,
                   TIME = seq(0, 22, length.out = 60)) %>%
  mutate(learning = (TIME == 0),
         `I(age^2)` = age^2,
         `age:TIME` = age*TIME) %>%
  dplyr::select(names(coef(fit))[-c(1,2)]) 
#here, the covariates are summed to the intercept ><remod
pred_10_fixed <- as.numeric(coef(fit)[1]) +
                                 as.numeric(t(as.vector(coef(fit)[-c(1,2)])) %*%
                                 unlist(t(as.matrix(nDF))))
#normal/hearing loss
pred_20_fixed <- as.numeric(coef(fit)[2]) +
                                 as.numeric(t(as.vector(coef(fit)[-c(1,2)])) %*%
                                 unlist(t(as.matrix(nDF))))
nDF <- nDF %>%
  mutate(pred_10 = expit(pred_10_fixed),
         pred_20 = expit(pred_20_fixed))
p1 <- nDF %>% pivot_longer(cols = c(pred_10,
                              pred_20),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(probability = ifelse(str_detect(Type, "10"),
                              "Normal|Excellent",
                              "Hearing Loss|\nExcellent")) %>%
  ggplot() + geom_line(aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = as.factor(age),
                           lty = probability)) +
  ylim(0,1) + theme_bw() + 
  scale_color_discrete(name = "Age")
p2 <- nDF %>%
  mutate(Excellent = pred_10,
         Normal = pred_20 - pred_10,
         Hearingloss = 1 - pred_20) %>%
  pivot_longer(cols = c(Excellent, Normal, Hearingloss),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(Type = factor(as.factor(Type), 
                       levels = c('Excellent', 'Normal', 'Hearingloss'), 
                       labels = c('Excellent', 'Normal', 'Hearing loss'), 
                       ordered = TRUE)) %>%
  ggplot() + geom_line(aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = as.factor(age))) +
  theme_bw() + ylab("Probability prediction") +
  facet_grid(cols = vars(Type)) + 
  theme(legend.position = "none")
p1 + p2 + 
  plot_layout(widths = c(1, 2))

```


## Random-effects model

On top of the fixed effects (equation \@ref(eq:Ordinal)), the random effects model includes a random intercept for each subject, estimated with the *clmm* function from the **ordinal** package. Random slopes were not included since a model with random slopes did not converge. The covariate $age$ was also standardized and centered to improve convergence. Thereforse, the odds are not directly comparable to the odds in Table \@ref(tab:geetable).

Here we again use a greedy approach and for the model selection criterion we use the $AIC$ and come up with the model in equation \@ref(eq:Ordinal-MM). 

\begin{equation}
\left\{
                \begin{array}{ll}
                  logit[P(Y_i\leq Excellent|x_i)] = \alpha_1 + \beta_{1}age_i +  \beta_{2}TIME_i + \beta_{3}learning_i + \\ \hspace{5cm} \beta_{4}age^2_i + \beta_{5}age_i*TIME_i + b_i\\
                  logit[P(Y_i\leq Normal|x_i)] = \alpha_2 + \beta_{1}age_i + \beta_{2}TIME_i + \beta_{3}learning_i + \\ \hspace{5cm} \beta_{4}age^2_i + \beta_{5}age_i*TIME_i + b_i \\
                  b_i \sim N(0,\sigma^2)
                \end{array}
              \right.
(\#eq:Ordinal-MM)
\end{equation}



```{r fitremod, echo=FALSE, message = FALSE, cache=TRUE}
remod <- clmm(y_discrete ~ age_scale*TIME + learning + I(age_scale^2) +
               (1|id_integer), #doesn't work with random slopes
             data = data)
# remod2 <- clmm2(y_discrete ~ age_scale*TIME + learning + I(age_scale^2),
#                 random = id_integer, #doesn't work with random slopes
#              data = data,
#              Hess = TRUE)
```

```{r retable, echo=FALSE, message = FALSE}
sre <- summary(remod)$coefficients 
sre %>% as.data.frame %>%
  mutate(Estimate = ifelse(str_detect(rownames(sre), "Normal"),
                           Estimate,
                           -1*Estimate), #to make them comparable to the gee coefficients
         var = c("$\\alpha_1$", "$\\alpha_2$", "age$_{scale}$", "TIME",
                 "learning", "age$^2_{scale}$", "age$_{scale}$*TIME"),
         estimate = sprintf("%.2f %s", Estimate, stars.pval(`Pr(>|z|)`)),
         odds = round(exp(Estimate),2)) %>%
  dplyr::select(var, estimate, odds) %>%
  kable(booktabs = TRUE,
        caption = "Estimated mixed effects model",
        row.names = FALSE,
        escape = FALSE,
        format = "latex",
        col.names = c("Parameter", "Estimate", "Odds"))
```

The random intercept has a variance (standard deviation) of `r round(remod$ST$id_integer[1]^2,2)` (`r round(remod$ST$id_integer[1],2)`).

To infer the marginal evolution of hearing loss over time, one cannot simply set the random intercept equal to zero to calculate the prediction. This is because the expectation of a logit function is not equal to the logit of the expectation. Figure \@ref(fig:REpredictions) shows the evolutions for the average subjects of a certain age (where $b_i=0$) to the marginal evolutions (integrated GLMM). The latter are used to get marginal predictions for each of the ordinal levels in Figure \@ref(fig:REmarginalpredictions). This figure clearly shows the learning effect at $TIME==0$. The youngest subjects have the highest probability of having excellent hearing and that probability goes down as time progresses. For older subjects, there seems to be a tipping point where the probability of having normal hearing starts to go down, as the probability of having hearing loss steeply increases. 

```{r REpredictions, echo=FALSE, message = FALSE, fig.width = 6, fig.height = 4, fig.cap = "Marginal predictions versus the evolutions for the average subjects"}
agecat <- c(20,30,40,50,60,70)
scaled_ages <- scale(agecat,
                       center = attr(data$age_scale,"scaled:center"),
                       scale = attr(data$age_scale,"scaled:scale"))[,1]
nDF <- expand.grid(age_scale = scaled_ages,
                   TIME = seq(0, 22, length.out = 60)) %>%
  mutate(learning = (TIME == 0),
         `I(age_scale^2)` = age_scale^2,
         `age_scale:TIME` = age_scale*TIME) %>%
  dplyr::select(names(coef(remod))[-c(1,2)]) 
#excellent|normal Note that it's beta[1] !-! SUM(other coefficients*covaraites) see expression (1) in https://cran.r-project.org/web/packages/ordinal/vignettes/clm_article.pdf
pred_excellent_normal_fixed <- as.numeric(coef(remod)[1]) -
                                 as.numeric(t(as.vector(coef(remod)[-c(1,2)])) %*%
                                 unlist(t(as.matrix(nDF))))
#normal/hearing loss
pred_normal_hl_fixed <- as.numeric(coef(remod)[2]) -
                                 as.numeric(t(as.vector(coef(remod)[-c(1,2)])) %*%
                                 unlist(t(as.matrix(nDF))))
#number of randomly drawn numbers
n <- 2000
randomnumbers <- rnorm(n, mean = 0, sd = remod$ST$id_integer[1])
nDF <- nDF %>%
  mutate(pred_excellent_normal_naive = expit(pred_excellent_normal_fixed),
         pred_excellent_normal = sapply(1:nrow(nDF), 
                                    FUN = function(x){
                                      mean(
                                        expit(
                                          pred_excellent_normal_fixed[x] +
                                            randomnumbers))}),
         pred_excellent_normal_error = sapply(1:nrow(nDF), 
                                    FUN = function(x){
                                      qnorm(0.975) * sd(
                                        expit(
                                          pred_excellent_normal_fixed[x] +
                                            randomnumbers)) / sqrt(n)}),
         pred_normal_hl_naive = expit(pred_normal_hl_fixed),
         pred_normal_hl = sapply(1:nrow(nDF), 
                                    FUN = function(x){
                                      mean(
                                        expit(
                                          pred_normal_hl_fixed[x] +
                                            randomnumbers))}),
         pred_normal_hl_error = sapply(1:nrow(nDF), 
                                    FUN = function(x){
                                      qnorm(0.975) * sd(
                                        expit(
                                          pred_normal_hl_fixed[x] +
                                            randomnumbers)) / sqrt(n)}),
         age = factor(as.factor(age_scale), levels = scaled_ages,
                      labels = agecat))
nDF %>% pivot_longer(cols = c(pred_excellent_normal_naive,
                              pred_excellent_normal,
                              pred_normal_hl_naive,
                              pred_normal_hl),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(naive = str_detect(Type, "naive"),
         probability = ifelse(str_detect(Type, "normal_hl"),
                              "Normal|Hearing Loss",
                              "Excellent|Normal")) %>%
  ggplot() + geom_line(aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = age,
                           lty = probability,
                           alpha = naive)) +
  scale_alpha_manual(breaks = c(TRUE, FALSE), values = c(0.4, 1), name = "", 
                     labels = c("Evolutions average subjects", "Marginal evolution")) +
  ylim(0,1) + theme_bw()
```

```{r REmarginalpredictions, echo=FALSE, message = FALSE, fig.width = 6, fig.height = 4, fig.cap= "Marginal evolutions"}
nDF %>%
  mutate(Excellent = pred_excellent_normal,
         Normal = pred_normal_hl - pred_excellent_normal,
         Hearingloss = 1 - pred_normal_hl) %>%
  pivot_longer(cols = c(Excellent, Normal, Hearingloss),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(Type = factor(as.factor(Type), 
                       levels = c('Excellent', 'Normal', 'Hearingloss'), 
                       labels = c('Excellent', 'Normal', 'Hearing loss'), 
                       ordered = TRUE)) -> pred
pred %>% ggplot() + geom_line(aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = age)) +
  theme_bw() + ylab("Probability prediction") +
  facet_grid(cols = vars(Type)) + 
  theme(legend.position = "bottom")
```


### Empirical Bayes prediction

```{r EBestimates, echo=FALSE, message = FALSE, fig.width = 6, fig.height = 4, fig.cap= "Empirical Bayes (EB) estimates"}
ranef <- data.frame(ranef = remod$ranef,
                    id = as.factor(1:length(remod$ranef)))
maxfollowup <- data %>% group_by(id) %>%
  summarize(maxfollowup = max(TIME),
            nbvisits = n()) %>%
  ungroup()
p1 <- data %>% left_join(ranef) %>% 
  ggplot() + geom_histogram(aes(x = ranef,
                                y = (..count..)/sum(..count..)),
                            bins = 50) +
  theme_bw() + xlab("Empirical bayes estimate \nfor the random intercept") + 
  ylab("Percentage") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1L)) 
p2 <- data %>% left_join(ranef) %>% 
  left_join(maxfollowup) %>%
  ggplot() + geom_point(aes(x=ranef, y = age, 
                            color = maxfollowup),
                        alpha = 0.7) +
  geom_vline(aes(xintercept = 0))  + theme_bw() +
  xlab("Empirical bayes estimate \nfor the random intercept") +
  ylab('Age at the start of the study') +
  scale_color_continuous(name = "Follow-up time (years)")+
  theme(legend.position = "bottom")

p1 + p2 


outliers <- data %>% left_join(ranef) %>% 
  left_join(maxfollowup) %>%
  filter(abs(ranef) > 5) %>%
  group_by(id, age, maxfollowup) %>%
  summarize(nbvisits = n()) %>%
  ungroup()
```

Apart from the marginal evolution, we can also get empirical bayes estimates for all subjects.  Figure \@ref(fig:EBestimates) shows the distribution of the random intercept on the left and the scatterplot on the right shows how the random intercepts are related to the subject's age and follow-up time. 
It can be seen that there are some outliers with high random intercepts, meaning that these subjects have higher than expected hearing threshold i.e. a higher than expected probability of hearing loss. There are `r nrow(outliers)` subjects with EB estimate > 5. `r sum(outliers$maxfollowup>10)` of these subjects were followed up more than 10 years and they range from age `r min(outliers$age)` to `r max(outliers$age)`. In the current dataset, no clear reason can be find as to why these subjects deviate so much from expectation. It would be interesting to try to link the EB estimates to other characteristics that might influence hearing such as occupation for example.

Figure \@ref(fig:EBestimates-individuals) shows the individually predicted evolution for a subset of subjects that have an age that is at most 1 year older or younger than the shown marginal evolutions. 

```{r EBestimates-individuals, echo=FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 4, fig.cap= "The dashed lines are predictions for individuals in the dataset (including random effect). Full lines are the marginal evolutions."}
# we pick all individuals that are at most one year younger or one year older than the focus ages that were chosen.
data %>%
  filter(age>=agecat-1 & age<=agecat+1) %>%
  mutate(age = round(age/10)*10) %>%
  mutate(`I(age_scale^2)` = age_scale^2,
         `age_scale:TIME` = age_scale*TIME) %>%
  left_join(ranef) %>% 
  mutate(pred_excellent_normal = 
           expit(as.numeric(coef(remod)[1]) -
                   coef(remod)[3] * age_scale - 
                   coef(remod)[4] * TIME -
                   coef(remod)[5] * learning - 
                   coef(remod)[6] * `I(age_scale^2)` - 
                   coef(remod)[7] * `age_scale:TIME`-
                   ranef) 
           ,
         pred_normal_hl = 
           expit(as.numeric(coef(remod)[2]) -
                   coef(remod)[3] * age_scale - 
                   coef(remod)[4] * TIME -
                   coef(remod)[5] * learning - 
                   coef(remod)[6] * `I(age_scale^2)` - 
                   coef(remod)[7] * `age_scale:TIME` - 
                   ranef),
         Excellent = pred_excellent_normal,
         Normal = pred_normal_hl - pred_excellent_normal,
         Hearingloss = 1 - pred_normal_hl) %>%
  pivot_longer(cols = c(Excellent, Normal, Hearingloss),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(Type = factor(as.factor(Type), 
                       levels = c('Excellent', 'Normal', 'Hearingloss'), 
                       labels = c('Excellent', 'Normal', 'Hearing loss'), 
                       ordered = TRUE)) -> id_predictions


ggplot() + geom_line(data = pred, aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = as.factor(age)), size = 1.2) +
  geom_line(data = id_predictions, aes(y = Prediction,
                           x = TIME, group = id,
                           color = as.factor(age)), lty = 2) +
  theme_bw() + ylab("Probability prediction") +
  facet_grid(cols = vars(Type)) + 
  theme(legend.position = "bottom") +
  scale_color_discrete(name = "Age")
```


```{r MixedModels-indiv-pred, echo=FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 4, fig.cap= "Predictions for all of the individuals."}
data %>%
  cbind(remod$fitted.values) %>%
  ggplot(aes(x=(age+TIME),y=`remod$fitted.values`,group = interaction(y_discrete, id),color=y_discrete))+
  geom_line(size=0.9)+
  labs(title ="CLMM Probabilities",x="Age+TIME",y="Probability")+
  guides(color=guide_legend(title="Hearing"))+
  scale_color_manual(values = c("green","red","blue"))+
  theme_bw()
```

As is evident from figures \@ref(fig:REmarginalpredictions) and \@ref(fig:MixedModels-indiv-pred), the results of this analysis confirm the formal results from the previous section: the probability of maintaining excellent hearing decreases over time and by extension the probability of developing hearing loss increases over time. Additionally, different age groups have varying baseline hearing capacities; with baseline hearing thresholds decreasing sharply as they age. As was the case in the previous assignment, the learning effect appears to artificially deflate the patients' observed hearing capabilities for their first record; lowering their probability of having excellent hearing and thus inflating their probability of having normal/reduced hearing. These results are then supported by figure \@ref(fig:MixedModels-indiv-pred) which details the calculated relation between Age, Time, and an individual patient's probability of having a given level of hearing capacity according to the CLMM model. This plot clearly shows various hearing evolutions clustered according to the patients respective age and length of participation.

Figure \@ref(fig:EBestimates) details the empirically measured distribution of the calculated random effects. From these figures, we observe that the empirical distribution of random effects agrees with the assumption of well formed normality; lacking excessive skew or kurtosis. Additionally, the figures also show that the random intercept is largely independent of both starting age and time spent in the study. Confirming results from the previous section, these findings show that starting age and time based effects would be best fit as fixed effects.


## Transition model
Finally, we consider transition models. Transition models represent a natural way model categorical data, while both marginal and mixed effects model do so in a circumspect way.  Transition models are a special case of conditional models where a measurement $Y_{ij}$ in a longitudinal sequence is considered as a function of previous outcomes or history $h_{ij} = (Y_{i1},Y_{i2},....,Y_{i,j-1})$. In transition models, we consider the longitudinal data as a stochastic process with the appropriate Markov assumption, and from there we can use standard likelihood methodology to characterize the transition probabilities. Transition models in non-gaussian case can be given as:

\[Y_{ij} = \mu_{ij}^c + \epsilon_{ij}^c\]

where $\mu_{ij}^c$ is the conditional mean given by $E(Y_{ij}/h_{ij})$ and $\epsilon_{ij}$ is the noise or the stochastic component.
This is actually formulated as a Generalized Linear Model conditional on the previous outcomes and their parameters can be estimated using maximum likelihood and where joint likelihood is formed from the conditionals $Y_{ij}/h_{ij}$. Since the outcome is conditioned in the past, by chain rule the outcomes will be independent which helps to simplify the likelihood. Considering our case, transition models will be extremely useful when our interest is to see what will happen to the categorical responses from one moment to another. Since the main research questions are related to the long-term evolution of hearing thresholds and not predicting the next measurement based on the current one, transition models are likely not ideal conceptually.

Technically, there are also some hurdles if we were to implement a transition model. They require a balanced dataset (with equal amount of measurements for each subject) for accurate analysis. The dataset we are dealing with is unbalanced. One would needs to be balance with other techniques such as multiple imputation. Another issue we face while trying to use transition models on our data is the irregular spacing of the measurements [@de2017discrete].


# Discussion

In this assignment we have analyzed the hearing data using a Generalized Estimating Equations approach with marginal log odds and a Cumulative Link Model with mixed effects. Then, we have conducted a confirmatory analysis using an empirical Bayes methodology. Finally, we discussed the viability of transition models.  
First, we had to trichotomize the hearing data in order for it to be treated as an ordinal multinomial variable. We have selected our cut off points based expert evaluation and the existing medical literature used to diagnose hearing loss and have decided on the following values. If the hearing threshold is less than or equal to 6dB then the subject is classified as having *Excellent* hearing, if the threshold is between 6dB and 25dB then they are classified as having *Normal* hearing and if the threshold is more than 25 dB they are classified as having *hearing loss*. The majority of the subjects in dataset are healthy and only around 6% of all observations have hearing loss.

In the next step of the analysis, we evaluated the data using a Generalized Estimating Equations approach. The main drivers of hearing loss are the subjects' age, with a small additional learning effect from the first time they take the evaluation. We observe that, as people age, they are more likely to transition from having excellent hearing to having normal hearing, while the probability of hearing loss becomes tangible only above the age of 50. The learning effect constitutes a slight increase in probability of the subjects to be classified into a lower category if it is their first time being measured.
 	
We also conduct a Cumulative Link Model with mixed effects. This type of model has the advantage that it can be considered in a sense as continuous model where the different ordinal categories are cut of points of a latent continuous variable. Model convergence sometimes becomes an issue with these models. That is a logit-link model with random effects will create a mixed likelihood that combines the Normal distribution of the random effect with the Logistic distribution assumed for the latent responses. We end up with conditional probabilities for each subject, and come to the same conclusion as before, that is younger people are more likely to have Excellent hearing, with the probability of having Normal hearing increasing as they get older, and the probability of hearing loss becoming tangible after 50 years of age.


As expected, the marginal model and the mixed effects model gave similar results in terms of how the covariates influence hearing loss. It is clear that the two models show difference in the scale of their coefficients, i.e. the marginal probability coming from the GEE model and the conditional probability coming from the CLMM model. These differences arise from the fact that the mean of a nonlinear function of a random variable does not equal the nonlinear function of the mean. When we compare the subject specific curves to the population averaged curves it is clear that the slopes of the former are steeper than those of the latter. This characterizes further that the CLMM model results describe changes in the odds for an individual from the population, while the GEE model describes the population odds.

It is important to note that there is a difference between the interpretation of the marginal and mixed effects models. The marginal model is more useful when we are interested in results pertaining to the entire population, while mixed models are more suited to answering question regarding an individual in said population. The choice of model should therefore be motivated by the research question.
	


# Bibliography

