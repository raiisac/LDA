---
documentclass: article
fontsize: 12pt
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: 
    toc: false
    latex_engine: xelatex
    fig_caption: yes
    includes:
      in_header: preamble.sty
      before_body: titlepage.sty
bibliography: references.bib  
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven) # to read SAS files
library(kableExtra)
library(qwraps2)
library(rprojroot)
library(patchwork)
library(psych)
library(gtools)
library(ordinal)

knitr::opts_chunk$set(echo = TRUE)
```

```{r load, include=FALSE, message=FALSE, warning=FALSE}
trichotomization <- c(-100, 6, 25, 120)
data <- read_sas(find_root_file("data/hearing500lr.sas7bdat",
                                criterion = has_file("LDA.Rproj"))) %>%
  mutate(side = as.factor(side),
         side_integer = as.integer(side),
         TIME_scale = unname(scale(TIME, center = TRUE, scale = TRUE)),
         id = as.factor(id),
         id_integer = as.integer(id),
         age_measurement = age + TIME,
         age_scale = unname(scale(age, center = TRUE, scale = TRUE)),
         age_discrete = cut(age,
                            breaks = c(0,30,50,70,100),
                            labels = c("<30", "30-50", "50-70",">70")),
         y_discrete = factor(as.factor(cut(y,
                          breaks = trichotomization,
                          labels = c("Excellent", "Normal", "Hearing loss"))),
                          levels = c("Excellent", "Normal", "Hearing loss"),
                          ordered = TRUE),
         y_integer = as.integer(y_discrete),
         learning = 1*(TIME == 0)) %>%
  arrange(id)
mediansplit <- cut_number(data$y, n = 3)

expit <- function(x){1/(1 + exp(-x))}
```

# Data trichotomization

To trichotomize the data, suitable cut-off points need to be found. The cutoff points are often chosen based on either  expert knowledge or so as to optimize predictive power. An easy, often used method for dichotomization is a median-split since it assures that there are an equal amount of observation at either side of the cut-off value. Similarly, for trichotomization, we could aim for approximately 33.33% of the observations in each of the three categories. That would result in the following three categories: `r sprintf("%s, %s, %s",levels(mediansplit)[1],levels(mediansplit)[2],levels(mediansplit)[3])`.

It is quite common in literature to dichotomize hearing loss into normal hearing (\leq25 dB) and hearing loss (>25 dB) [see @garinis2017cumulative; @gallagher2019; @ju2022long, for example]. However, thichotomization is less common and it should be noted that it is generally not advised to discretize continuous data since some information is inevitably lost [@doi:10.1080/03610926.2016.1248783; @maccallum2002practice]. 

[The Centers for Disease Control and Prevention](https://www.cdc.gov/niosh/mining/userfiles/works/pdfs/2008-102.pdf) distinguishes the following levels of hearing loss, based on @clark1981:

- \leq 25 dB: Normal hearing
- 26 - 40 dB: Mild hearing loss
- 41 - 55 dB: Moderate hearing loss
- 56 - 70 dB: Moderate / severe hearing loss
- 71 - 90 dB: Severe hearing loss
- \geq 91 dB: Profound hearing loss


```{r clark, echo=FALSE, message = FALSE, fig.cap= "Hearing threshold over time, divided by left and right ear. The numbers in the bottom show the number of measurements that were taken.", fig.width = 5, fig.height=4}
data_clark <- data %>% mutate(
  y_clark = as.factor(cut(y, breaks = c(-13, 25, 40, 55, 56, 70, 90, 120)))
) %>%
  group_by(y_clark) %>%
  summarize(n = n(),
            n_id = n_distinct(id),
            avg_age = mean(age_measurement),
            nperc = n/nrow(data)*100) %>%
  ungroup() %>% 
  mutate(Cum = cumsum(n)/sum(n)*100) %>%
  dplyr::select(y_clark, n, nperc, Cum, n_id, avg_age) 
data_clark %>%
  kable(col.names = c("Category", "Nb observations", "Percentage", 
                      "Cumulative percentage", "Nb subjects", "Avg age"),
        caption = "Number of observations in each pre-defined categories from Clark (1981).",
        booktabs = TRUE,
        digits = 2) %>%
  kable_styling(latex_options = "HOLD_position")
```

Table \@ref(tab:clark) shows that, in this dataset, there is no one in the severe hearing loss categories and the large majority has normal hearing (`r round(data_clark[1, "nperc"],2)`%). The median for all observation with normal hearing (\leq 25dB) is 6 dB. We therefor suggest to trichotomize the data into the following categories:

- \leq 6 dB: Excellent hearing
- 7 - 25 dB: Normal hearing
- \geq 25 dB: Hearing loss

```{r ydiscrete, echo=FALSE, message = FALSE, fig.cap= "Hearing threshold over time, divided by left and right ear. The numbers in the bottom show the number of measurements that were taken.", fig.width = 5, fig.height=4}
data %>% 
  group_by(y_discrete) %>%
  summarize(n = n(),
            n_id = n_distinct(id),
            avg_age = mean(age_measurement),
            nperc = n/nrow(data)*100) %>%
  ungroup() %>% 
  mutate(Cum = cumsum(n)/sum(n)*100) %>%
  dplyr::select(y_discrete, n, nperc, Cum, n_id, avg_age) %>%
  kable(col.names = c("Category", "Nb observations", "Percentage", 
                      "Cumulative percentage", "Nb subjects", "Avg age"),
        caption = "Number of observations in each category.",
        booktabs = TRUE,
        digits = 2) %>%
  kable_styling(latex_options = "HOLD_position")
```

# Methodology

As discussed in the previous section, the dependent variable will be split up into three categories. As such, the dependent variable is tranformed from a continuous (integer) variable into an ordinal one where excellent hearing is the lowest level and hearing loss is the highest.

In congruence with previous analysis, the ordinal model will be proportional odds logistic regression:

\begin{equation}
\left\{
                \begin{array}{ll}
                  logit[P(Y_i\leq Excellent|x_i)] = \alpha_1 + \beta_{1}age_i +  \beta_{2}TIME_i + \beta_{3}learning_i + \\ \hspace{5cm} \beta_{4}age^2_i + \beta_{5}age_i*TIME_i \\
                  logit[P(Y_i\leq Normal|x_i)] = \alpha_2 + \beta_{1}age_i + \beta_{2}TIME_i + \beta_{3}learning_i + \\ \hspace{5cm} \beta_{4}age^2_i + \beta_{5}age_i*TIME_i 
                \end{array}
              \right.
(\#eq:Ordinal)
\end{equation}

All analysis was done in R. All scripts are freely available at [this git repository](https://github.com/raiisac/LDA). 


# Results

## Marginal model
First, we fit a marginal model with the *ordLORgee* from the **multgee**. This function allows for an ordinal dependent variable which is appropriate for our data. The result is shown in Table \@ref(tab:geetable). 

```{r loadgee, echo=FALSE, message = FALSE, warning = FALSE, eval = file.exists("Stefan RMD/fit.Rdata")}
#if the R object exists, load it to avoid long calculation times. If not, fit the model on the fly and same the object
library(multgee)
load("Stefan RMD/fit_gee.Rdata")
```

```{r fitgee, echo=FALSE, message = FALSE, eval = !file.exists("Stefan RMD/fit.Rdata")}
#if the R object exists, load it to avoid long calculation times. If not, fit the model on the fly and same the object
library(multgee)
fit <- ordLORgee(formula = y_discrete ~ age*TIME + I(age^2) +learning,
                 link = "logit", id = id, data = data,
                 LORstr = "uniform")#category.exch"
```

```{r geetable, echo=FALSE, message = FALSE}
sgee <- summary(fit)$coefficients 
sgee <- sgee[c(1:4,6,5, 7),] #swap learning and age squared to be the same as in the mixed model
sgee %>% as.data.frame %>%
  mutate(var = c("$\\alpha_1$", "$\\alpha_2$", rownames(sgee)[3:5], 
                 "age$^2$", rownames(sgee)[7]),
         estimate = sprintf("%.2f %s", Estimate, stars.pval(`Pr(>|san.z|)`)),
         odds = round(exp(Estimate),2)) %>%
  dplyr::select(var, estimate, odds) %>%
  kable(booktabs = TRUE,
        caption = "Estimated GEE model",
        row.names = FALSE,
        escape = FALSE,
        format = "latex",
        col.names = c("Parameter", "Estimate", "Odds"))
```

```{r geepredictions, echo=FALSE, message = FALSE, fig.width = 12, fig.height = 7, fig.cap = "Predictions from the marginal model."}
agecat <- c(20,30,40,50,60,70)
nDF <- expand.grid(age = agecat,
                   TIME = seq(0, 22, length.out = 60)) %>%
  mutate(learning = (TIME == 0),
         `I(age^2)` = age^2,
         `age:TIME` = age*TIME) %>%
  dplyr::select(names(coef(fit))[-c(1,2)]) 
#age:TIME is not yet in fit

nDF <- expand.grid(age = agecat,
                   TIME = seq(0, 22, length.out = 60)) %>%
  mutate(learning = (TIME == 0),
         `I(age^2)` = age^2,
         `age:TIME` = age*TIME) %>%
  dplyr::select(names(coef(fit))[-c(1,2)]) 
#here, the covariates are summed to the intercept ><remod
pred_10_fixed <- as.numeric(coef(fit)[1]) +
                                 as.numeric(t(as.vector(coef(fit)[-c(1,2)])) %*%
                                 unlist(t(as.matrix(nDF))))
#normal/hearing loss
pred_20_fixed <- as.numeric(coef(fit)[2]) +
                                 as.numeric(t(as.vector(coef(fit)[-c(1,2)])) %*%
                                 unlist(t(as.matrix(nDF))))
nDF <- nDF %>%
  mutate(pred_10 = expit(pred_10_fixed),
         pred_20 = expit(pred_20_fixed))
p1 <- nDF %>% pivot_longer(cols = c(pred_10,
                              pred_20),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(probability = ifelse(str_detect(Type, "10"),
                              "Normal|Excellent",
                              "Hearing Loss|\nExcellent")) %>%
  ggplot() + geom_line(aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = as.factor(age),
                           lty = probability)) +
  ylim(0,1) + theme_bw() + 
  scale_color_discrete(name = "Age")
p2 <- nDF %>%
  mutate(Excellent = pred_10,
         Normal = pred_20 - pred_10,
         Hearingloss = 1 - pred_20) %>%
  pivot_longer(cols = c(Excellent, Normal, Hearingloss),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(Type = factor(as.factor(Type), 
                       levels = c('Excellent', 'Normal', 'Hearingloss'), 
                       labels = c('Excellent', 'Normal', 'Hearing loss'), 
                       ordered = TRUE)) %>%
  ggplot() + geom_line(aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = as.factor(age))) +
  theme_bw() + ylab("Probability prediction") +
  facet_grid(cols = vars(Type)) + 
  theme(legend.position = "none")
p1 + p2 + 
  plot_layout(widths = c(1, 2))

```


## Random-effects model

On top of the fixed effects (equation \@ref(eq:Ordinal)), the random effects model only includes a random intercept since it did not converge with random slopes included. The covariate $age$ was also standardized and centered to improve convergence.

```{r fitremod, echo=FALSE, message = FALSE, cache=TRUE}
remod <- clmm(y_discrete ~ age_scale*TIME + learning + I(age_scale^2) +
               (1|id_integer), #doesn't work with random slopes
             data = data)
# remod2 <- clmm2(y_discrete ~ age_scale*TIME + learning + I(age_scale^2),
#                 random = id_integer, #doesn't work with random slopes
#              data = data,
#              Hess = TRUE)
```

```{r retable, echo=FALSE, message = FALSE}
sre <- summary(remod)$coefficients 
sre %>% as.data.frame %>%
  mutate(var = c("$\\alpha_1$", "$\\alpha_2$", "age$_{scale}$", "TIME",
                 "learning", "age$^2_{scale}$", "age$_{scale}$*TIME"),
         estimate = sprintf("%.2f %s", Estimate, stars.pval(`Pr(>|z|)`)),
         odds = round(exp(Estimate),2)) %>%
  dplyr::select(var, estimate, odds) %>%
  kable(booktabs = TRUE,
        caption = "Estimated mixed effects model",
        row.names = FALSE,
        escape = FALSE,
        format = "latex",
        col.names = c("Parameter", "Estimate", "Odds"))
```

The random intercept has a variance (standard deviation) of `r round(remod$ST$id_integer[1]^2,2)` (`r round(remod$ST$id_integer[1],2)`).



### Empirical Bayes prediction

To infer the marginal evolution of hearing loss over time, one cannot simply set the random intercept equal to zero to calculate the prediction. This is because the expectation of a logit function is not equal to the logit of the expectation. Figure \@ref(fig:REpredictions) shows the evolutions for the average subjects of a certain age (where $b_i=0$) to the marginal evolutions (integrated GLMM). The latter are used to get marginal predictions for each of the ordinal levels in Figure \@ref(fig:REmarginalpredictions). This figure clearly shows the learning effect at $TIME==0$. The youngest subject have the highest probability of having excellent hearing and that probability goes down as time progresses. For older subjects, there seems to be a tipping point where the probability of having normal hearing starts to go down, as the probability of having hearing loss steeply increases. 

```{r REpredictions, echo=FALSE, message = FALSE, fig.width = 6, fig.height = 4, fig.cap = "Marginal predictions versus the evolutions for the average subjects"}
agecat <- c(20,30,40,50,60,70)
scaled_ages <- scale(agecat,
                       center = attr(data$age_scale,"scaled:center"),
                       scale = attr(data$age_scale,"scaled:scale"))[,1]
nDF <- expand.grid(age_scale = scaled_ages,
                   TIME = seq(0, 22, length.out = 60)) %>%
  mutate(learning = (TIME == 0),
         `I(age_scale^2)` = age_scale^2,
         `age_scale:TIME` = age_scale*TIME) %>%
  dplyr::select(names(coef(remod))[-c(1,2)]) 
#excellent|normal Note that it's beta[1] !-! SUM(other coefficients*covaraites) see expression (1) in https://cran.r-project.org/web/packages/ordinal/vignettes/clm_article.pdf
pred_excellent_normal_fixed <- as.numeric(coef(remod)[1]) -
                                 as.numeric(t(as.vector(coef(remod)[-c(1,2)])) %*%
                                 unlist(t(as.matrix(nDF))))
#normal/hearing loss
pred_normal_hl_fixed <- as.numeric(coef(remod)[2]) -
                                 as.numeric(t(as.vector(coef(remod)[-c(1,2)])) %*%
                                 unlist(t(as.matrix(nDF))))
#number of randomly drawn numbers
n <- 2000
randomnumbers <- rnorm(n, mean = 0, sd = remod$ST$id_integer[1])
nDF <- nDF %>%
  mutate(pred_excellent_normal_naive = expit(pred_excellent_normal_fixed),
         pred_excellent_normal = sapply(1:nrow(nDF), 
                                    FUN = function(x){
                                      mean(
                                        expit(
                                          pred_excellent_normal_fixed[x] +
                                            randomnumbers))}),
         pred_excellent_normal_error = sapply(1:nrow(nDF), 
                                    FUN = function(x){
                                      qnorm(0.975) * sd(
                                        expit(
                                          pred_excellent_normal_fixed[x] +
                                            randomnumbers)) / sqrt(n)}),
         pred_normal_hl_naive = expit(pred_normal_hl_fixed),
         pred_normal_hl = sapply(1:nrow(nDF), 
                                    FUN = function(x){
                                      mean(
                                        expit(
                                          pred_normal_hl_fixed[x] +
                                            randomnumbers))}),
         pred_normal_hl_error = sapply(1:nrow(nDF), 
                                    FUN = function(x){
                                      qnorm(0.975) * sd(
                                        expit(
                                          pred_normal_hl_fixed[x] +
                                            randomnumbers)) / sqrt(n)}),
         age = factor(as.factor(age_scale), levels = scaled_ages,
                      labels = agecat))
nDF %>% pivot_longer(cols = c(pred_excellent_normal_naive,
                              pred_excellent_normal,
                              pred_normal_hl_naive,
                              pred_normal_hl),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(naive = str_detect(Type, "naive"),
         probability = ifelse(str_detect(Type, "normal_hl"),
                              "Normal|Hearing Loss",
                              "Excellent|Normal")) %>%
  ggplot() + geom_line(aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = age,
                           lty = probability,
                           alpha = naive)) +
  scale_alpha_manual(breaks = c(TRUE, FALSE), values = c(0.4, 1), name = "", 
                     labels = c("Evolutions average subjects", "Marginal evolution")) +
  ylim(0,1) + theme_bw()
```

```{r REmarginalpredictions, echo=FALSE, message = FALSE, fig.width = 6, fig.height = 4, fig.cap= "Marginal evolutions"}
nDF %>%
  mutate(Excellent = pred_excellent_normal,
         Normal = pred_normal_hl - pred_excellent_normal,
         Hearingloss = 1 - pred_normal_hl) %>%
  pivot_longer(cols = c(Excellent, Normal, Hearingloss),
                     values_to = "Prediction",
                     names_to = "Type") %>%
  mutate(Type = factor(as.factor(Type), 
                       levels = c('Excellent', 'Normal', 'Hearingloss'), 
                       labels = c('Excellent', 'Normal', 'Hearing loss'), 
                       ordered = TRUE)) %>%
  ggplot() + geom_line(aes(y = Prediction,
                           x = TIME, group = interaction(Type, age),
                           color = age)) +
  theme_bw() + ylab("Probability prediction") +
  facet_grid(cols = vars(Type)) + 
  theme(legend.position = "bottom")
```


```{r EBestimates, echo=FALSE, message = FALSE, fig.width = 6, fig.height = 4, fig.cap= "Empirical Bayes (EB) estimates"}
ranef <- data.frame(ranef = remod$ranef,
                    id = as.factor(1:length(remod$ranef)))
maxfollowup <- data %>% group_by(id) %>%
  summarize(maxfollowup = max(TIME),
            nbvisits = n()) %>%
  ungroup()
p1 <- data %>% left_join(ranef) %>% 
  ggplot() + geom_histogram(aes(x = ranef,
                                y = (..count..)/sum(..count..)),
                            bins = 50) +
  theme_bw() + xlab("Empirical bayes estimate \nfor the random intercept") + 
  ylab("Percentage") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1L)) 
p2 <- data %>% left_join(ranef) %>% 
  left_join(maxfollowup) %>%
  ggplot() + geom_point(aes(x=ranef, y = age, 
                            color = maxfollowup),
                        alpha = 0.7) +
  geom_vline(aes(xintercept = 0))  + theme_bw() +
  xlab("Empirical bayes estimate \nfor the random intercept") +
  ylab('Age at the start of the study') +
  scale_color_continuous(name = "Follow-up time (years)")+
  theme(legend.position = "bottom")

p1 + p2 


outliers <- data %>% left_join(ranef) %>% 
  left_join(maxfollowup) %>%
  filter(abs(ranef) > 5) %>%
  group_by(id, age, maxfollowup) %>%
  summarize(nbvisits = n()) %>%
  ungroup()
```

Apart from the marginal evolution, we can also get empirical bayes estimates for all subjects.  Figure \@ref(fig:EBestimates) shows the distribution of the random intercept on the left and the scatterplot on the right shows how the random intercepts related to the subject's age and follow-up time. 
It can be seen that there are more outliers on the right, meaning that these subjects have higher than expected hearing threshold i.e. a higher than expected probability of hearing loss. There are `r nrow(outliers)` subjects with EB estimate > 5. `r sum(outliers$maxfollowup>10)` of these subjects were followed up more than 10 years and they range from age `r min(outliers$age)` to `r max(outliers$age)`. In the current dataset, no clear reason can be find as to why these subjects deviate so much from expectation. It would be interesting to try to link the EB estimates to other characteristics that might influence hearing such as occupation for example.


## Transition model


# Discussion


# Bibliography

