---
documentclass: article
fontsize: 12pt
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: 
    toc: false
    latex_engine: xelatex
    fig_caption: yes
    includes:
      in_header: preamble.sty
      before_body: titlepage.sty
bibliography: references.bib  
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven) # to read SAS files
library(kableExtra)
library(qwraps2)
library(rprojroot)
library(patchwork)
library(psych)
library(gtools)
library(ordinal)
library(visdat)#visualize missingness
library(mice)
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, include=FALSE, message=FALSE, warning=FALSE}
trichotomization <- c(-100, 6, 25, 120)
data <- read_sas(find_root_file("data/hearing500lr.sas7bdat",
                                criterion = has_file("LDA.Rproj"))) %>%
  mutate(side = as.factor(side),
         side_integer = as.integer(side),
         TIME_discrete = round(TIME),
         id = as.factor(id),
         id_integer = as.integer(id),
         age_measurement = age + TIME,
         age_scale = unname(scale(age, center = TRUE, scale = TRUE)),
         age_discrete = cut(age,
                            breaks = c(0,30,50,70,100),
                            labels = c("<30", "30-50", "50-70",">70")),
         y_discrete = factor(as.factor(cut(y,
                          breaks = trichotomization,
                          labels = c("Excellent", "Normal", "Hearing loss"))),
                          levels = c("Excellent", "Normal", "Hearing loss"),
                          ordered = TRUE),
         y_integer = as.integer(y_discrete),
         learning = 1*(TIME == 0)) %>%
  arrange(id)
subject_characteristics <- data %>%
  group_by(id) %>%
  summarize(age = first(age),
            age_scale = first(age_scale),
            age_discrete = first(age_discrete),
            id_integer = first(id_integer)) %>%
  ungroup()
  
data_grid <- expand.grid(id = unique(data$id),
                         TIME_discrete = 0:max(data$TIME_discrete),
                         side = unique(data$side)) %>%
  left_join(data %>% dplyr::select(-age, -age_scale, -age_discrete, 
                                   -id_integer)) %>%
  mutate(R = is.na(y), # R denotes missingness
         learning = (TIME_discrete == 0)) %>%
  left_join(subject_characteristics, by = c("id" = "id")) #add the subject characteristics back in)
```

# Introduction

This report assesses the evolution of hearing thresholds over time for a sample of `r nlevels(data$id)` healthy male volunteers. The data originates from the famous Baltimore Longitudinal Study of Aging (BLSA). Previous research showed a change in hearing threshold for all age groups but especially the older population [@doi:10.1121/1.399731]. In this report, we will especially take care of missingness in the data.

First, the *TIME* variable is rounded to the nearest integer value. As such, we aim to balance the dataset with equally-spaced time instances when hearing thresholds are measured. 

Hearing thresholds will be explored, both as a continuous variable and as a trichotomized (ordinal) variable with the following three levels:

- $\leq$ 6 dB: Excellent hearing
- over 6 and $\leq$ 25 dB: Normal hearing
- $\geq$ 26 dB: Hearing loss

## Missingness exploration
```{r Missingnessprep, include=FALSE, message=FALSE, warning=FALSE}
data_by_id_time <- data_grid %>% 
  group_by(id, TIME_discrete) %>%
  summarize(n = sum(!is.na(y_discrete))) %>%
  ungroup()
```

After discretizing the *TIME* variable, we consider a subject to be missing at a certain time instance if there is no measurement for that subject at that time. It should be noted that, if the subject is not missing (`r round((1-sum(data_by_id_time$n==0)/nrow(data_by_id_time))*100,2)`% of TIME-subject instances), we usually (`r round((sum(data_by_id_time$n==2)/nrow(data_by_id_time))*100,2)`% of TIME-subject instances) have two measurements (one for each ear) at each time instance. In fact, the average number of measurements per subject at each time instance is `r round(mean(data_by_id_time$n),2)`, and maximum `r round(max(data_by_id_time$n),2)`. 

Figure \@ref(fig:Missingness) was created using the *visdat* package. It shows all subjects, ordered from youngest (in the top) to oldest (in the bottom) and whether or not their data is missing at a certain time instance (on the x-axis). The percentages on top shows the percentage of missingness at each time instance. It is clear from Figure \@ref(fig:Missingness) that the missingness is not monotone; subjects may be missing at one time instance and come back later. Since there are too many possible missingness patterns with 23 time instances ($2^{23}$), we do not give an overview of the number of subjects that follow each possible pattern. Instead, figure \@ref(fig:missingnessovertime) shows, for each time instance, the number of subjects that:

- are *present*: when the subject's hearing is measured at time $t$ and $t-1$
- are *missing*: when the subject is missing at time $t$ and $t-1$
- *drop out*: when the subject's hearing is measured at time $t-1$ but not at time$t$
- *return*: when the subject's hearing is measured at time $t$ but not at time $t-1$

Figure \@ref(fig:missingnessovertime) clearly shows that subject rarely are measured two years in a row, most are not measured at $t=1$, and the number of subjects that stay missing gradually increases as time passes.

 
```{r Missingness, include=TRUE, echo = FALSE, message=FALSE, warning=FALSE, fig.cap = "Visual inspection of missingness for different ages at different time instances.", fig.height = 9, fig.width=7}
data_grid %>%
  mutate(TIME_discrete = factor(as.factor(TIME_discrete), ordered = TRUE)) %>%
  group_by(id, TIME_discrete) %>%
  summarize(missing = ifelse(any(!is.na(y)),TRUE, NA),
        age = first(age)) %>%
  ungroup() %>%
  pivot_wider(names_from = TIME_discrete, values_from = missing) %>% 
  arrange(age) %>%
  dplyr::select(3:(3 + max(data_grid$TIME_discrete))) %>%
  vis_miss() + ylab("id sorted by age") +
  theme(plot.margin = margin(0, 1, 0, 0, "cm")) 
```

```{r missingnessovertime, include=TRUE, echo = FALSE, message=FALSE, warning=FALSE, fig.cap = "Number of subjects the are present, return, drop out or are missing at each time instance.", fig.height = 4, fig.width=6}
categorize <- function(x){
  cat <- rep(NA, length(x))
  cat[1] <- ifelse(x[1], "missing", "present")
  if(length(x) > 1){
    l <- lag(x)
    cat[-1] <- ifelse(l[-1],
                      ifelse(x[-1],
                             "missing",
                             "return"),
                      ifelse(x[-1],
                             "drop out",
                             "present"))
  }
  return(cat)
}

data_grid %>%
  group_by(id, TIME_discrete) %>%
  summarize(R = any(R)) %>%
  ungroup() %>%
  group_by(id) %>%
  mutate(missing = factor(as.factor(categorize(R)), ordered = TRUE,
                          levels = c("present", "return", 
                          "drop out", "missing"))) %>%
  ungroup() %>%
  ggplot() +
  geom_bar(aes(x = TIME_discrete, fill = missing)) +
  scale_fill_discrete(name = "") +
  ylab("Number of subjects") + xlab("TIME") +
  theme_bw()
```

Lastly we explore whether the missingness can be explained by the data by fitting a mixed model to a dataset where $R_it$ is equal to one if the hearing threshold is missing and zero otherwise:

\begin{equation}
\left\{
                \begin{array}{ll}
                logit(R_{it}) = \beta_0 + \beta_1 TIME_{it} + \beta_3 side_{it} + \beta_4 age_{it} + \beta_5 R_{it-1} + b_i \\
                b_i \sim N(0,\sigma^2)
                \end{array}
          \right.
(\#eq:missingness)
\end{equation}

The variable *age* was standardized to get convergence in the model. Table \@ref(tab:missingmixedmodel) shows that TIME is significant; as time increases, subjects are more likely to be missing. We can therefore assume missingness at random (MAR). Left ear measurements are also more likely to be missing. A subject is also less likely to be missing at time $t$ if he was missing at time $t-1$. This can be seen especially in the first couple of years in figure \@ref(fig:Missingness): all but 3 subjects are measured at $t=0$, almost no-one is measured at $t=1$ and many are measured again at $t=2$.

```{r missingmixedmodel, include=TRUE, echo = FALSE, message=FALSE, warning=FALSE, eval=TRUE}
library(lme4)
data_grid <- data_grid %>% mutate(
  R_num = 1*(R == TRUE),
  age_scale = scale(age, center = TRUE, scale = TRUE)) %>%
  group_by(id) %>%
  mutate(R_num_lag = dplyr::lag(R_num, n = 1),
         y_last = tail(y[!is.na(y)],1)) %>%
  ungroup()
m1 <- glmer(formula = R_num ~ TIME_discrete + side + age_scale + R_num_lag + (1|id),
        data = data_grid,
        family = binomial(link = "logit"))
s <- summary(m1) 
s$coefficients %>% as.data.frame() %>%
  mutate(var = c('Intercept', "TIME", "sideright", "age", "$R_{t-1}$"),
         result = sprintf("%.2f %s", Estimate, stars.pval(`Pr(>|z|)`))) %>%
  dplyr::select(var, result) %>%
  rbind(c("sigma", round(unname(attr(s$varcor$id, "stddev"))^2,2))) %>%
  kable(booktabs = TRUE,
        caption = "A mixed model to predict missingness.",
        col.names = c('Variable', 'Estimate'),
        row.names = FALSE, 
        escape = FALSE) %>%
  kable_styling()
```


# Methodology

First, a direct likelihood analysis is compared with multiple imputation in the ?? continuous/discrete??? case. 
Next, weighted generalized estimating equations are compared with ‘multiple-imputation generalized estimating equations’. 
Lastly, a sensitivity analysis is performed.

For imputation, the *mice* library is used [@JSSv045i03] and different imputation techniques were tested: Predictive mean matching, Bayesian linear regression, Unconditional mean imputation, and imputation by random forests. 

All analysis was done in R. All scripts are freely available at [this git repository](https://github.com/raiisac/LDA). 

# Results

## Direct likelihood analysis versus multiple imputation

For the purposes of this section we will analyze the hearing data respective to the continuous response. Two popular ways to analyze missing data is to use either a direct likelihood method or an analysis through multiple imputation. In a direct likelihood analysis missing values are imputed according to the general likelihood function defined on a per patient basis. The R package *lavaan* allows for the direct calculation for the patients’ likelihood functions through the use of their provided structural equation modeling techniques. This method does have its drawbacks, namely that patients which exhibit 0 variance in their hearing threshold evolution cannot have their likelihood functions calculated appropriately. To resolve this issue we removed the grouping factor from the modeling procedure. This approach does however make the direct likelihood method unappealing, as we lose the information defining the individual patient. This issue is then compounded with the models inability to be cross examined as the modeling procedure uses up all available degrees of freedom.

```{r message=FALSE, include=FALSE}
#direct lh (lavaan FIML)
#data_grid$age_measurement=data_grid$age+data_grid$TIME_discrete


library(lavaan)

fit.dlh.cont=sem('y~age+TIME_discrete+learning+age:TIME_discrete',data=data_grid,missing="direct",fixed.x=FALSE)
sum.dlh=summary(fit.dlh.cont)
```

```{r}
library(tidySEM)
# layoutmat=matrix(rep(NA,15),nrow=3)
# layoutmat[1,3]="y"
# layoutmat[2,2]="learning"
# layoutmat[2,4]="age:Time_discrete"
# layoutmat[3,3]="age"
# layoutmat[3,5]="Time_discrete"
#get_layout(fit.dlh.cont)
sem_graph=graph_sem(fit.dlh.cont)
sem_graph
```

```{r include=FALSE}
# #MI
# data_grid_cutdown=data_grid%>%transmute(id,y,TIME_discrete,age,learning)
# 
# library(mice)
# data_grid_mice_norm=complete(mice(data_grid_cutdown,method = "norm"))
# 
# data_grid_mice_pmm=complete(mice(data_grid_cutdown,method = "pmm"))
# 
# data_grid_mice_mean=complete(mice(data_grid_cutdown,method = "mean"))
# 
# data_grid_mice_rf=complete(mice(data_grid_cutdown,method = "rf"))
```

To address the issues around direct likelihood analysis we adopt multiple imputation methods. Here artificial data points are generated according to the algorithms: Predictive mean matching, Bayesian linear regression, Unconditional mean imputation, and imputation by random forests. Being generated from an aggregation of 10 simulations, these simulated data points are then used in conjunction with the real data to draft appropriate models. These models were then compared against each other with the following summary statistics detailing the winning model according to AIC. 


```{r include=FALSE}
load(find_root_file("data/imputated_data.Rdata", 
                    criterion = has_file("LDA.Rproj")))
```

```{r include=FALSE}
data_imp_mean=data_imp_mean%>%rowwise()%>%mutate(impfullMED=median(c(impfull1,impfull2,impfull3,impfull4,impfull5,impfull6,impfull7,impfull8,impfull9,impfull10)))

data_imp_norm=data_imp_norm%>%rowwise()%>%mutate(impfullMED=median(impfull1,impfull2,impfull3,impfull4,impfull5,impfull6,impfull7,impfull8,impfull9,impfull10))

data_imp_pmm=data_imp_pmm%>%rowwise()%>%mutate(impfullMED=median(impfull1,impfull2,impfull3,impfull4,impfull5,impfull6,impfull7,impfull8,impfull9,impfull10))

data_imp_rf=data_imp_rf%>%rowwise()%>%mutate(impfullMED=median(impfull1,impfull2,impfull3,impfull4,impfull5,impfull6,impfull7,impfull8,impfull9,impfull10))

```

```{r include=FALSE}
library(nlme)

lmeControl(maxIter = 1000)

#fit.mi.norm.cont.simp=lme(y ~ age*TIME_discrete + learning + I(age^2),data = data_grid_mice_norm,random = ~1|id)

fit.mi.pmm.cont.simp=lme(impfullMED ~ age*TIME_discrete + learning + I(age^2),data = data_imp_pmm,random = ~1|id)

fit.mi.mean.cont.simp=lme(impfullMED ~ age*TIME_discrete + learning + I(age^2) ,data = data_imp_mean,random = ~1|id)

fit.mi.mean.cont.simp.v2=lme(impfullMED ~ age*TIME_discrete + learning ,data = data_imp_mean,random = ~1|id)

fit.mi.norm.cont.simp=lme(impfullMED ~ age*TIME_discrete + learning + I(age^2),data = data_imp_norm,random = ~1|id)

fit.mi.rf.cont.simp=lme(impfullMED ~ age*TIME_discrete + learning + I(age^2),data = data_imp_rf,random = ~1|id)

#fit.mi.rf.cont.simp=lme(y ~ age_scale*TIME_discrete + learning + I(age_scale^2),data = data_grid_mice_rf,random = ~1|id)
```

```{r include=FALSE}
sum.mi.pmm.cont.simp=summary(fit.mi.pmm.cont.simp)
sum.mi.mean.cont.simp=summary(fit.mi.mean.cont.simp)
sum.mi.mean.cont.simp.v2=summary(fit.mi.mean.cont.simp.v2)
sum.mi.cont.cont.simp=summary(fit.mi.norm.cont.simp)
sum.mi.rf.cont.simp=summary(fit.mi.rf.cont.simp)
```

```{r include=FALSE}
sum.mi.pmm.cont.simp$AIC
sum.mi.mean.cont.simp$AIC
sum.mi.mean.cont.simp.v2$AIC
sum.mi.cont.cont.simp$AIC
sum.mi.rf.cont.simp$AIC
```

```{r include=FALSE}
#fit.mi.norm.cont.compsymm=lme(y ~ age_scale*TIME_discrete + learning + I(age_scale^2),data = data_grid_mice_norm,random = ~1|id,cor = nlme::corCompSymm())

fit.mi.pmm.cont.compsymm=lme(impfullMED ~ age*TIME_discrete + learning + I(age^2),data = data_imp_pmm,random = ~1|id,cor = nlme::corCompSymm())

fit.mi.mean.cont.compsymm=lme(impfullMED ~ age*TIME_discrete + learning + I(age^2),data = data_imp_mean,random = ~1|id,cor = nlme::corCompSymm())

fit.mi.norm.cont.compsymm=lme(impfullMED ~ age*TIME_discrete + learning + I(age^2),data = data_imp_norm,random = ~1|id,cor = nlme::corCompSymm())

fit.mi.rf.cont.compsymm=lme(impfullMED ~ age*TIME_discrete + learning + I(age^2),data = data_imp_rf,random = ~1|id,cor = nlme::corCompSymm())

#fit.mi.rf.cont.compsymm=lme(y ~ age_scale*TIME_discrete + learning + I(age_scale^2),data = data_grid_mice_rf,random = ~1|id,cor = nlme::corCompSymm())
```

```{r include=FALSE}
sum.mi.pmm.cont.compsymm=summary(fit.mi.pmm.cont.compsymm)
sum.mi.mean.cont.compsymm=summary(fit.mi.mean.cont.compsymm)
sum.mi.cont.cont.compsymm=summary(fit.mi.norm.cont.compsymm)
sum.mi.rf.cont.compsymm=summary(fit.mi.rf.cont.compsymm)
```

```{r include=FALSE}
sum.mi.pmm.cont.compsymm$AIC
sum.mi.mean.cont.compsymm$AIC
sum.mi.cont.cont.compsymm$AIC
sum.mi.rf.cont.compsymm$AIC
```


```{r}
finmodsum=sum.mi.mean.cont.simp.v2$tTable[,c(1,2,5)]
round(finmodsum,3)
```

Q4

### Direct likelihood

### Multiple imputation



## Weighted generalized estimating equations versus ‘multiple-imputation generalized estimating equations’
Using frequentist methods,
Q5

## Sensitivity analysis

Q6

# Bibliography

